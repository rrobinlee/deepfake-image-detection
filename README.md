# Facial Segmentation and Deepfake Image Detection

The dataset contains 70k real faces from a Flickr dataset collected by Nvidia, as well as 70k fake faces. The fake faces are a small sample of 1 million faces generated by StyleGAN. The data is further divided into training, validation, and testing sub-folders, which is a standard practice for machine learning projects. Each split contains 50% real faces and 50% fake faces, and all images have already been resized to 256px for consistency. We have:

* Train Split: 100,000 total images
* Valid Split: 20,000 total images
* Test Split:  20,000 total images

Each train, validation, and test subfolder is divided into two folders:

* **real_faces**: Contains images of genuine human faces.
* **fake_faces**: Contains images of faces generated by an AI model, commonly known as deep fakes.

We also included a picture of Queen Padme Amidala from Star Wars for fun.

|Kaggle Dataset|Star Wars Example|
|-|-|
|<img width="437.805929919" height="321" alt="image" src="https://github.com/user-attachments/assets/c7132845-779f-4fd4-b244-2d452647c66f" />|<img width="260" height="321" alt="image" src="https://github.com/user-attachments/assets/71b623dd-5596-4ef1-929b-b5eed80e1684" />|


## Face Segmentation 

### Model 1: MediaPipe Face Landmarker

The MediaPipe Face Landmarker detects face landmarks and facial expressions in images and videos to identify human facial expressions, apply facial filters and effects, and create virtual avatars

Model Overview:
* Detection: Feeds the prepared image to the detector method. This runs the pre-trained model, containing the coordinates of 478 landmarks
* Landmark Extraction: Extracts the indices for the specific landmarks that form the left eye, right eye, nose, and lips. It then uses list comprehensions to pull the actual coordinate data for those features.
* Drawing: Takes a list of landmark coordinates and draws a small circle at each point on the image. This visualizes the model's output directly on the face.

|Star Wars Test|Dataset Test 1|Dataset Test 2|Dataset Test 3|Dataset Test 4|
|-|-|-|-|-|
|<img width="636" height="658" alt="image" src="https://github.com/user-attachments/assets/bf232402-494f-41ab-82c6-1eac4d8a8c2a" />|<img width="636" height="658" alt="image" src="https://github.com/user-attachments/assets/97665b39-4058-4ff7-8760-e828a96c7826" />|<img width="636" height="658" alt="image" src="https://github.com/user-attachments/assets/3a6a697a-e495-4387-b9e0-871a38da8e78" />|<img width="636" height="658" alt="image" src="https://github.com/user-attachments/assets/e678e452-f5c2-4be6-aa99-1c13a26b92a9" />|<img width="636" height="658" alt="image" src="https://github.com/user-attachments/assets/69edeff4-fbaa-45b1-8a05-568f17440d29" />|

|Dataset Test 5|Dataset Test 6|Dataset Test 7|Dataset Test 8|Dataset Test 9|
|-|-|-|-|-|
|<img width="636" height="658" alt="image" src="https://github.com/user-attachments/assets/60a300f4-2d19-4f47-9b1e-05d4d7bd24c6" />|<img width="636" height="658" alt="image" src="https://github.com/user-attachments/assets/c89115a8-b3af-4699-9f1a-95d53e5fd15f" />|<img width="636" height="658" alt="image" src="https://github.com/user-attachments/assets/26683d2b-6d5e-4760-8fbf-654db770a747" />|<img width="636" height="658" alt="image" src="https://github.com/user-attachments/assets/74cbf079-076d-40b1-9c35-084234e4343a" />|<img width="636" height="658" alt="image" src="https://github.com/user-attachments/assets/3782fabe-6b85-467d-a675-900b2c2bab6c" />|

### Model 2: Bilateral Segmentation Network (BiSeNet)

BiSeNet is a real-time semantic segmentation architecture designed to balance spatial detail and receptive field for efficient and accurate image segmentation.

|Star Wars Test|Dataset Test 1|Dataset Test 2|Dataset Test 3|Dataset Test 4|
|-|-|-|-|-|
|<img width="636" height="658" alt="image" src="https://github.com/user-attachments/assets/152a0fed-a4b2-4586-a957-1ddb800b5fb5" />|<img width="636" height="658" alt="image" src="https://github.com/user-attachments/assets/06976c8c-2ca9-4f2a-b835-13b756898e0b" />|<img width="636" height="658" alt="image" src="https://github.com/user-attachments/assets/2f757115-1839-4bf1-bc40-a7e02d607822" />|<img width="636" height="658" alt="image" src="https://github.com/user-attachments/assets/15c0e737-cd8c-4520-bc73-7c62be953d0c" />|<img width="636" height="658" alt="image" src="https://github.com/user-attachments/assets/d09daa40-330f-48c2-801a-2a40f9c9b983" />|

|Dataset Test 5|Dataset Test 6|Dataset Test 7|Dataset Test 8|Dataset Test 9|
|-|-|-|-|-|
|<img width="636" height="658" alt="image" src="https://github.com/user-attachments/assets/0e3b94ea-8068-46a9-a121-d0aec28e2ddc" />|<img width="636" height="658" alt="image" src="https://github.com/user-attachments/assets/54d510e4-173f-4d6d-a1d9-c9d7eef47922" />|<img width="636" height="658" alt="image" src="https://github.com/user-attachments/assets/ba5aaa86-94e2-4145-a863-d4de632b81e0" />|<img width="636" height="658" alt="image" src="https://github.com/user-attachments/assets/bee5689b-21bb-4659-8245-763dda077dab" />|<img width="636" height="658" alt="image" src="https://github.com/user-attachments/assets/fb3031ee-7e1f-4fc4-9166-52581803c093" />|


## Deep-Fake Classification Model Performance 

### Model 1: Transfer-Learn DenseNet121 CNN using Feature Extraction

In order to transfer learn a CNN, we take a pre-trained backbone (DenseNet121), remove the last fully-connected layer, and treat the rest of the CNN as a fixed feature extractor. 
* We freeze all the layers of the pre-trained model, remove its original classification head, add new classification layers, and then train **only** these new classification layers.
* The pre-trained weights in the base model are never updated

#### Performance

* Test Loss: 0.1996
* Test Accuracy: 0.9620
* Test Precision: 0.9803
* Test Recall: 0.9428
* Test F1-Score: 0.96
  
<Details>
<summary><b>Performance Visualizations</b></summary>
<img width="550" height="555" alt="image" src="https://github.com/user-attachments/assets/b6a95954-27b7-4ebd-9172-53ed40f33fa6" />

|ROC Curve|Precision-Recall Curve|
|-|-|
|<img width="681" height="555" alt="image" src="https://github.com/user-attachments/assets/103e3f94-ea28-4abe-ba6d-983a8aa5d586" />|<img width="681" height="555" alt="image" src="https://github.com/user-attachments/assets/5d5410c1-ae3d-4a72-b464-85fe81ff963b" />|
</Details>

### Model 2: Vision Transformer Classifier

The vision transformer applies the transformer architecture from NLP towards visual data. Rather than process text, the ViT will:

1. Cut the image into fixed-size patches. Each patch is treated as a “token”, similar to a word in a sentence.
2. Each token is processed using a Transformer encoder. Self-attention learns how each patch relates (or “talks”) to every other patch.
3. Encoder outputs a classification token and feeds it into an MLP to make the final prediction.

#### Performance

* Test Loss: 0.2425
* Test Accuracy: 0.9056
* Test Precision:0.9204
* Test Recall: 0.8879
* Test F1-Score: 0.9039

<Details>
<summary><b>Performance Visualizations</b></summary>
  
<img width="550" height="555" alt="image" src="https://github.com/user-attachments/assets/7e165cf3-e22b-46ac-8fa4-1573e71eb63a" />

|ROC Curve|Precision-Recall Curve|
|-|-|
|<img width="683" height="555" alt="image" src="https://github.com/user-attachments/assets/41b71151-f25b-4487-986f-60ca7d02cc65" />|<img width="683" height="555" alt="image" src="https://github.com/user-attachments/assets/8381cd52-ac6d-4620-aaab-495336f2dc47" />|

</Details>

### Performance Comparison

|Test Image|True Class|CNN-predicted Class|ViT-predicted class|
|-|-|-|-|
||||












