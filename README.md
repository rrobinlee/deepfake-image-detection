# Facial Segmentation and Deepfake Image Detection

The [Kaggle dataset](https://www.kaggle.com/datasets/xhlulu/140k-real-and-fake-faces) contains 70k real faces from a Flickr dataset collected by Nvidia, as well as 70k fake faces. The fake faces are a small sample of 1 million faces generated by StyleGAN. The data is further divided into training, validation, and testing sub-folders, which is a standard practice for machine learning projects. Each split contains 50% real faces and 50% fake faces, and all images have already been resized to 256px for consistency. We have:

* Train Split: 100,000 total images
* Valid Split: 20,000 total images
* Test Split:  20,000 total images

Each train, validation, and test subfolder is divided into two folders:

* **real_faces**: Contains images of genuine human faces.
* **fake_faces**: Contains images of faces generated by an AI model, commonly known as deep fakes.

We also included a picture of Queen Padme Amidala from Star Wars for fun.

|Kaggle Dataset|Star Wars Example|
|-|-|
|<img width="437.805929919" height="321" alt="image" src="https://github.com/user-attachments/assets/c7132845-779f-4fd4-b244-2d452647c66f" />|<img width="260" height="321" alt="image" src="https://github.com/user-attachments/assets/71b623dd-5596-4ef1-929b-b5eed80e1684" />|


## Face Segmentation 

### Model 1: MediaPipe Face Landmarker

The MediaPipe Face Landmarker detects face landmarks and facial expressions in images and videos to identify human facial expressions, apply facial filters and effects, and create virtual avatars

Model Overview:
* Detection: Feeds the prepared image to the detector method. This runs the pre-trained model, containing the coordinates of 478 landmarks
* Landmark Extraction: Extracts the indices for the specific landmarks that form the left eye, right eye, nose, and lips. It then uses list comprehensions to pull the actual coordinate data for those features.
* Drawing: Takes a list of landmark coordinates and draws a small circle at each point on the image. This visualizes the model's output directly on the face.

|Star Wars Test|Dataset Test 1|Dataset Test 2|Dataset Test 3|Dataset Test 4|
|-|-|-|-|-|
|<img width="636" height="658" alt="image" src="https://github.com/user-attachments/assets/bf232402-494f-41ab-82c6-1eac4d8a8c2a" />|<img width="636" height="658" alt="image" src="https://github.com/user-attachments/assets/97665b39-4058-4ff7-8760-e828a96c7826" />|<img width="636" height="658" alt="image" src="https://github.com/user-attachments/assets/3a6a697a-e495-4387-b9e0-871a38da8e78" />|<img width="636" height="658" alt="image" src="https://github.com/user-attachments/assets/e678e452-f5c2-4be6-aa99-1c13a26b92a9" />|<img width="636" height="658" alt="image" src="https://github.com/user-attachments/assets/69edeff4-fbaa-45b1-8a05-568f17440d29" />|

|Dataset Test 5|Dataset Test 6|Dataset Test 7|Dataset Test 8|Dataset Test 9|
|-|-|-|-|-|
|<img width="636" height="658" alt="image" src="https://github.com/user-attachments/assets/60a300f4-2d19-4f47-9b1e-05d4d7bd24c6" />|<img width="636" height="658" alt="image" src="https://github.com/user-attachments/assets/c89115a8-b3af-4699-9f1a-95d53e5fd15f" />|<img width="636" height="658" alt="image" src="https://github.com/user-attachments/assets/26683d2b-6d5e-4760-8fbf-654db770a747" />|<img width="636" height="658" alt="image" src="https://github.com/user-attachments/assets/74cbf079-076d-40b1-9c35-084234e4343a" />|<img width="636" height="658" alt="image" src="https://github.com/user-attachments/assets/3782fabe-6b85-467d-a675-900b2c2bab6c" />|

### Model 2: Bilateral Segmentation Network (BiSeNet)

BiSeNet is a real-time semantic segmentation architecture designed to balance spatial detail and receptive field for efficient and accurate image segmentation.

|Star Wars Test|Dataset Test 1|Dataset Test 2|Dataset Test 3|Dataset Test 4|
|-|-|-|-|-|
|<img width="636" height="658" alt="image" src="https://github.com/user-attachments/assets/152a0fed-a4b2-4586-a957-1ddb800b5fb5" />|<img width="636" height="658" alt="image" src="https://github.com/user-attachments/assets/06976c8c-2ca9-4f2a-b835-13b756898e0b" />|<img width="636" height="658" alt="image" src="https://github.com/user-attachments/assets/2f757115-1839-4bf1-bc40-a7e02d607822" />|<img width="636" height="658" alt="image" src="https://github.com/user-attachments/assets/15c0e737-cd8c-4520-bc73-7c62be953d0c" />|<img width="636" height="658" alt="image" src="https://github.com/user-attachments/assets/d09daa40-330f-48c2-801a-2a40f9c9b983" />|

|Dataset Test 5|Dataset Test 6|Dataset Test 7|Dataset Test 8|Dataset Test 9|
|-|-|-|-|-|
|<img width="636" height="658" alt="image" src="https://github.com/user-attachments/assets/0e3b94ea-8068-46a9-a121-d0aec28e2ddc" />|<img width="636" height="658" alt="image" src="https://github.com/user-attachments/assets/54d510e4-173f-4d6d-a1d9-c9d7eef47922" />|<img width="636" height="658" alt="image" src="https://github.com/user-attachments/assets/ba5aaa86-94e2-4145-a863-d4de632b81e0" />|<img width="636" height="658" alt="image" src="https://github.com/user-attachments/assets/bee5689b-21bb-4659-8245-763dda077dab" />|<img width="636" height="658" alt="image" src="https://github.com/user-attachments/assets/fb3031ee-7e1f-4fc4-9166-52581803c093" />|


## Deep-Fake Classification Model Performance 

### Model 1: Transfer-Learn DenseNet121 CNN using Feature Extraction/Fine-Tuning

We use DenseNet121 pre-trained on ImageNet as a feature extractor by removing its original classification head. Initially, all backbone layers are frozen and only a new classification head is trained. For fine-tuning, we unfreeze the top 25% of backbone layers (keeping BatchNorm layers frozen) to adapt high-level features to the target dataset while preserving low-level representations.

Performance Metrics:
  
> Test Loss: 0.1948<br/>
> Test Accuracy: **0.9643**<br/>
> Test Precision: 0.9661<br/>
> Test Recall: 0.9624<br/>
> Test F1-Score: 0.9642<br/>
> Test AUPRC: 0.9941<br/>
> Test AUROC: 0.9944

<details>
<summary>Model Structure:</summary>

<br>

* Input: `Input(shape=(IMG_H, IMG_W, 3))`
* Data Augmentation (training only)
  * `RandomFlip("horizontal")` -> `RandomRotation(0.1)` -> `RandomZoom(0.1)` -> `RandomTranslation(0.1, 0.1)` -> `RandomContrast(0.1)`
* Preprocessing
  * `Rescaling(1/255.0)` -> normalize pixel values to [0,1]
  * `Normalization(mean=[0.485,0.456,0.406], variance=[0.229^2,0.224^2,0.225^2])` -> ImageNet-style normalization
* Backbone Model: **DenseNet121**
  * Pre-trained on ImageNet, were `include_top=False`
  * Stage 1: all layers frozen (fixed feature extractor)
  * Output: feature maps of approx. size (H/32, W/32, 1024) 
* Add new Classification Head
  * Flatten feature maps: `GlobalAveragePooling2D()`
  * Regularization: `BatchNormalization()` and `Dropout(0.5)` 
  * Fully connected layer: `Dense(256, activation='relu', kernel_regularizer=l2(1e-4))`
  * More regularization: `BatchNormalization()` and `Dropout(0.4)`
  * Binary classification output: `Dense(1, activation='sigmoid')`
* Training and fine-tuning
  * Stage 1 (Warm-up): train only the new classification head, backbone frozen
  * Stage 2 (Fine-tuning): unfreeze top 25% of backbone layers, keep BatchNorm layers frozen, lower learning rate for adaptation

</details>

Because of their local receptive fields, CNNs tend to require more fine-tuning and regularization in order to prevent overfitting.

### Model 2: Vision Transformer Classifier (Untuned parameters)

The vision transformer applies the transformer architecture from NLP towards visual data. Rather than process text, the ViT will:

1. Cut the image into fixed-size patches. Each patch is treated as a “token”, similar to a word in a sentence.
2. Each token is processed using a Transformer encoder. Self-attention learns how each patch relates (or “talks”) to every other patch.
   * Global attention mechanism allows modeling long-range dependencies.
4. Encoder outputs a classification token and feeds it into an MLP to make the final prediction.

Performance Metrics:

> Test Loss: 0.2425<br/>
> Test Accuracy: **0.9056**<br/>
> Test Precision:0.9204<br/>
> Test Recall: 0.8879<br/>
> Test F1-Score: 0.9039

<details>
<summary>Model Structure:</summary>

<img width="531" height="393" alt="image" src="https://github.com/user-attachments/assets/d97ac42e-a1a8-4d92-b1da-80f2384c8ec8" />

<br>

* Input: `(IMG_SIZE[0], IMG_SIZE[1], 3)` image
* Stage 1: Data Augmentation (train-time only)
  * Resize -> Rescale -> Random Flip -> Random Rotation -> Random Zoom
* Stage 2: Patch Extraction
  * `Patches` layer: Extracts non-overlapping 8×8 patches
  * Result: `NUM_PATCHES = (IMG_SIZE[0] // 8)^2` patches, each flattened
* Stage 3: Patch Encoding
  * Dense projection to 64-dim embedding (`PROJECTION_DIM`)
  * Learnable positional embedding of size (`NUM_PATCHES, 64`)
* Stage 4: Transformer Encoder Block × 4 (`TRANSFORMER_LAYERS = 4`)
  * For each block:
    1. LayerNorm
    2. Multi-Head Attention (6 heads, `key_dim = 64`, `dropout=0.1`)
    3. Skip connection
    4. LayerNorm
    5. MLP with `[128, 64]` units (from `TRANSFORMER_UNITS`) + GELU activations + `dropout=0.1`
    6. Skip connection
* Stage 5: Representation & Classification Head
  * LayerNorm
  * Global Average Pooling (token-level to vector)
  * `Dropout(0.2)`
  * `Dense(1024, GELU)`
  * `Dropout(0.2)`
  * `Dense(512, GELU)`
  * `Dropout(0.1)`
  * Final `Dense(1)` output

</details>

<details>
<summary>How to implement quantization:</summary>

<br>

Full model currently saved as `best_vit_model.h5`

<br>

  ```
from tensorflow import keras
# define custom layer classes (needed for vit)
custom_objects = {
    "Patches":Patches,
    "PatchEncoder": PatchEncoder
}
model = keras.models.load_model(
    "best_vit_model.h5",
    custom_objects=custom_objects
)
converter = tf.lite.TFLiteConverter.from_keras_model(model)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.target_spec.supported_ops = [
    tf.lite.OpsSet.TFLITE_BUILTINS, 
    tf.lite.OpsSet.SELECT_TF_OPS  
]
converter.target_spec.supported_types = [tf.float16]
quant_model = converter.convert()
with open("model_final_quantized.tflite", "wb") as f:
    f.write(quant_model)
```

</details>

ViTs are computationally expensive compared to CNNs and require large datasets for optimal performance. Due to limited compute units, this model is **not fine-tuned**. 

### Model Performance Comparison

#### Confusion Matrix

|CNN|ViT|
|-|-|
|<img width="550" height="701" alt="image" src="https://github.com/user-attachments/assets/e2b7e879-50d1-4810-8314-9a9ba251ad49" />|<img width="550" height="555" alt="image" src="https://github.com/user-attachments/assets/7e165cf3-e22b-46ac-8fa4-1573e71eb63a" />|

#### ROC Curve

|CNN|ViT|
|-|-|
|<img width="550" height="701" alt="image" src="https://github.com/user-attachments/assets/cb02e132-9297-42ef-bf6f-2beac3e7ca40" />|<img width="550" height="555" alt="image" src="https://github.com/user-attachments/assets/41b71151-f25b-4487-986f-60ca7d02cc65" />|

#### Precision-Recall Curve

|CNN|ViT|
|-|-|
|<img width="550" height="701" alt="image" src="https://github.com/user-attachments/assets/c032bf03-49aa-4570-af2a-b757a232026d" />|<img width="550" height="555" alt="image" src="https://github.com/user-attachments/assets/8381cd52-ac6d-4620-aaab-495336f2dc47" />|

#### Test Images
 
|Image|True Class|CNN Prediction|ViT Prediction|
|-|-|-|-|
|<img width="256" height="256" alt="image" src="https://github.com/user-attachments/assets/69319b42-8ba0-4549-9b00-5ad5eb83611f" />| Fake |Fake (confidence: 0.9861)|Fake (confidence: 0.9995)|
|<img width="256" height="256" alt="image" src="https://github.com/user-attachments/assets/8f5f8fb5-0d57-4864-bd44-635073f76966" />| Fake |Fake (confidence: 0.9838)|Fake (confidence: 0.9945)|
|<img width="256" height="256" alt="image" src="https://github.com/user-attachments/assets/676173e9-f465-444b-8599-4885bad58323" />| Fake |Fake (confidence: 0.9815)|Fake (confidence: 0.9985)|
|<img width="256" height="256" alt="image" src="https://github.com/user-attachments/assets/16ee519a-7209-48cd-ab69-0e0d628cd306" />| Fake |Fake (confidence: 0.9638)|Fake (confidence: 0.9991)|
|<img width="256" height="256" alt="image" src="https://github.com/user-attachments/assets/c644d363-c8ab-441f-a580-3664872340f4" />| Fake |Fake (confidence: 0.9760)|Fake (confidence: 0.9941)|
|<img width="256" height="256" alt="image" src="https://github.com/user-attachments/assets/1038268d-bda9-4bc7-b007-0b152e71f3e5" />| Real |Real (confidence: 0.6140)|Real (confidence: 0.9882)|
|<img width="256" height="256" alt="image" src="https://github.com/user-attachments/assets/70ce29a9-b6e8-43a8-b686-1ea5314948d8" />| Real |Real (confidence: 0.9431)|Real (confidence: 0.9535)|
|<img width="256" height="256" alt="image" src="https://github.com/user-attachments/assets/a4aa34b4-2a03-4065-8606-c581b8730bed" />| Real |Real (confidence: 0.9663)|Real (confidence: 0.9966)|
|<img width="260" height="321" alt="image" src="https://github.com/user-attachments/assets/1a6f6e61-e9a4-41e5-866b-3962457675d3" />| Real |Real (confidence: 0.9762)|Real (confidence: 1.0000)|
|<img width="256" height="256" alt="image" src="https://github.com/user-attachments/assets/a699d651-4a6a-441c-9cab-1372a155719c" />| Real |Real (confidence: 0.9755)|Real (confidence: 0.9970)|



